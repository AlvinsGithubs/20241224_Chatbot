from langchain.storage import LocalFileStore
import streamlit as st
import subprocess
import math
from pydub import AudioSegment
import glob
import openai
import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import StrOutputParser
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from dotenv import load_dotenv
import re
import logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# íŒŒì¼ ì´ë¦„ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def sanitize_filename(filename):
    return re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FFmpeg ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜
def check_ffmpeg_installed():
    try:
        subprocess.run(["ffmpeg", "-version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return True
    except FileNotFoundError:
        return False

# FFmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
if not check_ffmpeg_installed():
    st.error("FFmpegê°€ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜, PATHì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. FFmpegë¥¼ ì„¤ì¹˜í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    st.stop()

# pydubì— FFmpeg ê²½ë¡œ ì§ì ‘ ì„¤ì • (í•„ìš” ì‹œ)
AudioSegment.converter = r"C:\Users\AlvinJang\Desktop\report_Maker\ffmpeg\bin\ffmpeg.exe"

# OpenAI API í‚¤ ë¡œë“œ
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì— `OPENAI_API_KEY`ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()

llm = ChatOpenAI(
    temperature=0.1,
    openai_api_key=openai_api_key,  # API í‚¤ ë¡œë“œ
)

splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=800,
    chunk_overlap=100,
)

@st.cache_data()
def embed_file(file_path):
    try:
        file_name = os.path.basename(file_path)
        cache_dir = LocalFileStore(f"./.cache/embeddings/{file_name}")
        loader = TextLoader(file_path, encoding='utf-8')
        docs = loader.load_and_split(text_splitter=splitter)
        logger.info(f"Loaded {len(docs)} documents from {file_path}.")
        for i, doc in enumerate(docs):
            logger.info(f"Document {i+1}: {doc.page_content[:100]}...")
        embeddings = OpenAIEmbeddings()
        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
        vectorstore = FAISS.from_documents(docs, cached_embeddings)
        retriever = vectorstore.as_retriever()
        return retriever
    except Exception as e:
        logger.error(f"Error embedding file {file_path}: {e}")
        st.error(f"ì„ë² ë”© íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        raise e

@st.cache_data()
def transcribe_chunks(chunk_folder, destination):
    # ì „ì‚¬ëœ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì „ì‚¬í•˜ì§€ ì•ŠìŒ
    if os.path.exists(destination):
        logger.info(f"Transcript file {destination} already exists. Skipping transcription.")
        return
    files = glob.glob(f"{chunk_folder}/*.mp3")  # MP3 íŒŒì¼ë¡œ ìœ ì§€
    files.sort()
    for file in files:
        with open(file, "rb") as audio_file, open(destination, "a", encoding="utf-8") as text_file:
            try:
                transcript = openai.Audio.transcribe(
                    "whisper-1",
                    audio_file,
                )
                text_file.write(transcript["text"] + "\n")
                logger.info(f"Transcribed {file} successfully.")
            except Exception as e:
                logger.error(f"Error transcribing {file}: {e}")
                st.error(f"ì˜¤ë””ì˜¤ ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    # ì „ì‚¬ê°€ ì™„ë£Œëœ í›„ íŒŒì¼ í™•ì¸
    if os.path.exists(destination):
        file_size = os.path.getsize(destination)
        if file_size > 0:
            logger.info(f"Transcript file {destination} created successfully with size {file_size} bytes.")
        else:
            logger.error(f"Transcript file {destination} is empty.")
            st.error("ì „ì‚¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        logger.error(f"Transcript file {destination} does not exist.")
        st.error("ì „ì‚¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

@st.cache_data()
def extract_audio_from_video(video_path):
    audio_path = os.path.splitext(video_path)[0] + ".mp3"
    command = [
        "ffmpeg",
        "-y",
        "-i",
        video_path,
        "-vn",
        audio_path,
    ]
    try:
        subprocess.run(command, check=True)
        logger.info(f"Audio extracted successfully: {audio_path}")
        return audio_path
    except subprocess.CalledProcessError as e:
        logger.error(f"FFmpeg ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error(f"FFmpeg ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        raise e

@st.cache_data()
def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):
    if not os.path.exists(chunks_folder):
        os.makedirs(chunks_folder, exist_ok=True)
    try:
        track = AudioSegment.from_mp3(audio_path)
    except Exception as e:
        st.error(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        logger.error(f"Error loading audio file {audio_path}: {e}")
        return
    chunk_len = chunk_size * 60 * 1000  # ë¶„ì„ ë°€ë¦¬ì´ˆë¡œ ë³€í™˜
    chunks = math.ceil(len(track) / chunk_len)
    for i in range(chunks):
        start_time = i * chunk_len
        end_time = (i + 1) * chunk_len
        chunk = track[start_time:end_time]
        chunk_filename = f"chunk_{i}.mp3"
        try:
            chunk.export(
                os.path.join(chunks_folder, chunk_filename),
                format="mp3",
            )
            logger.info(f"Exported {chunk_filename}")
        except Exception as e:
            st.error(f"ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            logger.error(f"Error exporting chunk {chunk_filename}: {e}")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="MeetingGPT",
    page_icon="ğŸ’¼",
)

st.markdown(
    """
# MeetingGPT

Welcome to MeetingGPT! Upload a video file and I will provide you with a transcript, a summary, and a chatbot to ask any questions about it.

Get started by uploading a video file in the sidebar.
"""
)

with st.sidebar:
    video = st.file_uploader(
        "Video",
        type=["mp4", "avi", "mkv", "mov", "webm"],  # 'webm' ì¶”ê°€
    )

if video:
    chunks_folder = "./.cache/chunks"
    # .cache/chunks ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(chunks_folder, exist_ok=True)
    with st.status("Loading video...") as status:
        video_content = video.read()
        safe_video_name = sanitize_filename(video.name)
        video_path = os.path.abspath(os.path.join(".", ".cache", safe_video_name))
        transcript_path = os.path.splitext(video_path)[0] + ".txt"
        try:
            with open(video_path, "wb") as f:
                f.write(video_content)
            logger.info(f"Saved video file: {video_path}")
        except FileNotFoundError as e:
            st.error(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            logger.error(f"Error saving video file {video_path}: {e}")
            st.stop()
        status.update(label="Extracting audio...")
        try:
            audio_path = extract_audio_from_video(video_path)
        except Exception as e:
            st.error("ì˜¤ë””ì˜¤ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            logger.error(f"Audio extraction failed: {e}")
            st.stop()
        status.update(label="Cutting audio segments...")
        cut_audio_in_chunks(audio_path, 10, chunks_folder)
        status.update(label="Transcribing audio...")
        transcribe_chunks(chunks_folder, transcript_path)
    
    # ì „ì‚¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼ í™•ì¸
    if os.path.exists(transcript_path):
        if os.path.getsize(transcript_path) > 0:
            logger.info(f"Transcript file {transcript_path} exists and has content.")
        else:
            logger.error(f"Transcript file {transcript_path} is empty.")
            st.error("ì „ì‚¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        logger.error(f"Transcript file {transcript_path} does not exist.")
        st.error("ì „ì‚¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    transcript_tab, summary_tab, qa_tab = st.tabs(
        [
            "Transcript",
            "Summary",
            "Q&A",
        ]
    )

    with transcript_tab:
        if os.path.exists(transcript_path):
            if os.path.getsize(transcript_path) > 0:
                try:
                    with open(transcript_path, "r", encoding="utf-8") as file:
                        st.write(file.read())
                except Exception as e:
                    st.error(f"ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                    logger.error(f"Error reading transcript file {transcript_path}: {e}")
            else:
                st.write("ì „ì‚¬ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.write("ì „ì‚¬ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with summary_tab:
        start = st.button("Generate summary")
        if start:
            try:
                if not os.path.exists(transcript_path) or os.path.getsize(transcript_path) == 0:
                    st.error("ì „ì‚¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                else:
                    loader = TextLoader(transcript_path, encoding='utf-8')
                    docs = loader.load_and_split(text_splitter=splitter)

                    logger.info(f"Loaded {len(docs)} documents for summarization.")
                    for i, doc in enumerate(docs):
                        logger.info(f"Document {i+1}: {doc.page_content[:100]}...")

                    first_summary_prompt = ChatPromptTemplate.from_template(
                        """
                        Write a concise summary of the following:
                        "{text}"
                        CONCISE SUMMARY:                
                    """
                    )

                    first_summary_chain = first_summary_prompt | llm | StrOutputParser()

                    summary = first_summary_chain.invoke(
                        {"text": docs[0].page_content},
                    )

                    refine_prompt = ChatPromptTemplate.from_template(
                        """
                        Your job is to produce a final summary.
                        We have provided an existing summary up to a certain point: {existing_summary}
                        We have the opportunity to refine the existing summary (only if needed) with some more context below.
                        ------------
                        {context}
                        ------------
                        Given the new context, refine the original summary.
                        If the context isn't useful, RETURN the original summary.
                        """
                    )

                    refine_chain = refine_prompt | llm | StrOutputParser()

                    with st.status("Summarizing...") as status:
                        for i, doc in enumerate(docs[1:]):
                            status.update(label=f"Processing document {i+1}/{len(docs)-1} ")
                            summary = refine_chain.invoke(
                                {
                                    "existing_summary": summary,
                                    "context": doc.page_content,
                                }
                            )
                    st.write(summary)
            except Exception as e:
                st.error(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                logger.error(f"Error generating summary: {e}")

    with qa_tab:
        try:
            retriever = embed_file(transcript_path)
        except Exception as e:
            st.error(f"ì„ë² ë”© íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            logger.error(f"Error embedding file {transcript_path}: {e}")
            st.stop()

        # ì‚¬ìš©ìë¡œë¶€í„° ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
        user_question = st.text_input("Ask a question about the transcript:")
        if user_question:
            try:
                # ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
                docs = retriever.get_relevant_documents(user_question)
                if docs:
                    # ê´€ë ¨ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
                    context = "\n\n".join(doc.page_content for doc in docs)
                    qa_prompt = ChatPromptTemplate.from_template(
                        """
                        You are a helpful assistant.

                        Use the following context to answer the question.

                        Context:
                        {context}

                        Question:
                        {question}

                        Answer:
                    """
                    )
                    qa_chain = qa_prompt | llm
                    answer = qa_chain.invoke(
                        {
                            "context": context,
                            "question": user_question,
                        }
                    )
                    st.write(answer)
                else:
                    st.write("No relevant information found.")
            except Exception as e:
                st.error(f"Q&A ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                logger.error(f"Error during Q&A: {e}")



#-------ver1.0
# from langchain.storage import LocalFileStore
# import streamlit as st
# import subprocess
# import math
# from pydub import AudioSegment
# import glob
# import openai
# import os
# from langchain.chat_models import ChatOpenAI
# from langchain.prompts import ChatPromptTemplate
# from langchain.document_loaders import TextLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.schema import StrOutputParser
# from langchain.vectorstores.faiss import FAISS
# from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
# from dotenv import load_dotenv
# import re
# import logging
# from pydub import AudioSegment

# # FFmpeg ì‹¤í–‰ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ì§€ì •
# AudioSegment.converter = r"C:\\Users\\AlvinJang\\Desktop\\report_Maker\\ffmpeg\bin\\ffmpeg.exe"


# # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # íŒŒì¼ ì´ë¦„ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
# def sanitize_filename(filename):
#     return re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)

# # ë¡œê¹… ì„¤ì •
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# # FFmpeg ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜
# def check_ffmpeg_installed():
#     try:
#         subprocess.run(["ffmpeg", "-version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
#         return True
#     except FileNotFoundError:
#         return False

# # FFmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
# if not check_ffmpeg_installed():
#     st.error("FFmpegê°€ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜, PATHì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. FFmpegë¥¼ ì„¤ì¹˜í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
#     st.stop()

# # OpenAI API í‚¤ ë¡œë“œ
# openai_api_key = os.getenv("OPENAI_API_KEY")
# if not openai_api_key:
#     st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì— `OPENAI_API_KEY`ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
#     st.stop()

# llm = ChatOpenAI(
#     temperature=0.1,
#     openai_api_key=openai_api_key,  # API í‚¤ ë¡œë“œ
# )

# has_transcript = os.path.exists("./.cache/podcast.txt")

# splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
#     chunk_size=800,
#     chunk_overlap=100,
# )

# @st.cache_data()
# def embed_file(file_path):
#     file_name = os.path.basename(file_path)
#     cache_dir = LocalFileStore(f"./.cache/embeddings/{file_name}")
#     loader = TextLoader(file_path)
#     docs = loader.load_and_split(text_splitter=splitter)
#     embeddings = OpenAIEmbeddings()
#     cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
#     vectorstore = FAISS.from_documents(docs, cached_embeddings)
#     retriever = vectorstore.as_retriever()
#     return retriever

# @st.cache_data()
# def transcribe_chunks(chunk_folder, destination):
#     if has_transcript:
#         return
#     files = glob.glob(f"{chunk_folder}/*.mp3")  # MP3 íŒŒì¼ë¡œ ìœ ì§€
#     files.sort()
#     for file in files:
#         with open(file, "rb") as audio_file, open(destination, "a", encoding="utf-8") as text_file:
#             try:
#                 transcript = openai.Audio.transcribe(
#                     "whisper-1",
#                     audio_file,
#                 )
#                 text_file.write(transcript["text"] + "\n")
#                 logger.info(f"Transcribed {file} successfully.")
#             except Exception as e:
#                 logger.error(f"Error transcribing {file}: {e}")
#                 st.error(f"ì˜¤ë””ì˜¤ ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# @st.cache_data()
# def extract_audio_from_video(video_path):
#     if has_transcript:
#         return
#     audio_path = video_path.rsplit('.', 1)[0] + ".mp3"
#     command = [
#         "ffmpeg",
#         "-y",
#         "-i",
#         video_path,
#         "-vn",
#         audio_path,
#     ]
#     try:
#         subprocess.run(command, check=True)
#         logger.info(f"Audio extracted successfully: {audio_path}")
#     except subprocess.CalledProcessError as e:
#         logger.error(f"FFmpeg ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         st.error(f"FFmpeg ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#         raise e

# @st.cache_data()
# def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):
#     if has_transcript:
#         return
#     if not os.path.exists(chunks_folder):
#         os.makedirs(chunks_folder, exist_ok=True)
#     try:
#         track = AudioSegment.from_mp3(audio_path)
#     except Exception as e:
#         st.error(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#         logger.error(f"Error loading audio file {audio_path}: {e}")
#         return
#     chunk_len = chunk_size * 60 * 1000  # ë¶„ì„ ë°€ë¦¬ì´ˆë¡œ ë³€í™˜
#     chunks = math.ceil(len(track) / chunk_len)
#     for i in range(chunks):
#         start_time = i * chunk_len
#         end_time = (i + 1) * chunk_len
#         chunk = track[start_time:end_time]
#         chunk_filename = f"chunk_{i}.mp3"
#         try:
#             chunk.export(
#                 os.path.join(chunks_folder, chunk_filename),
#                 format="mp3",
#             )
#             logger.info(f"Exported {chunk_filename}")
#         except Exception as e:
#             st.error(f"ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#             logger.error(f"Error exporting chunk {chunk_filename}: {e}")

# # Streamlit í˜ì´ì§€ ì„¤ì •
# st.set_page_config(
#     page_title="MeetingGPT",
#     page_icon="ğŸ’¼",
# )

# st.markdown(
#     """
# # MeetingGPT

# Welcome to MeetingGPT! Upload a video file and I will provide you with a transcript, a summary, and a chatbot to ask any questions about it.

# Get started by uploading a video file in the sidebar.
# """
# )

# with st.sidebar:
#     video = st.file_uploader(
#         "Video",
#         type=["mp4", "avi", "mkv", "mov", "webm"],  # 'webm' ì¶”ê°€
#     )

# if video:
#     chunks_folder = "./.cache/chunks"
#     # .cache/chunks ë””ë ‰í† ë¦¬ ìƒì„±
#     os.makedirs("./.cache/chunks", exist_ok=True)
#     with st.status("Loading video...") as status:
#         video_content = video.read()
#         safe_video_name = sanitize_filename(video.name)
#         video_path = f"./.cache/{safe_video_name}"
#         audio_path = video_path.rsplit('.', 1)[0] + ".mp3"
#         transcript_path = video_path.rsplit('.', 1)[0] + ".txt"
#         try:
#             with open(video_path, "wb") as f:
#                 f.write(video_content)
#         except FileNotFoundError as e:
#             st.error(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#             logger.error(f"Error saving video file {video_path}: {e}")
#             st.stop()
#         status.update(label="Extracting audio...")
#         extract_audio_from_video(video_path)
#         status.update(label="Cutting audio segments...")
#         cut_audio_in_chunks(audio_path, 10, chunks_folder)
#         status.update(label="Transcribing audio...")
#         transcribe_chunks(chunks_folder, transcript_path)

#     transcript_tab, summary_tab, qa_tab = st.tabs(
#         [
#             "Transcript",
#             "Summary",
#             "Q&A",
#         ]
#     )

#     with transcript_tab:
#         if os.path.exists(transcript_path):
#             try:
#                 with open(transcript_path, "r", encoding="utf-8") as file:
#                     st.write(file.read())
#             except Exception as e:
#                 st.error(f"ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#                 logger.error(f"Error reading transcript file {transcript_path}: {e}")
#         else:
#             st.write("ì „ì‚¬ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

#     with summary_tab:
#         start = st.button("Generate summary")
#         if start:
#             try:
#                 loader = TextLoader(transcript_path)
#                 docs = loader.load_and_split(text_splitter=splitter)

#                 first_summary_prompt = ChatPromptTemplate.from_template(
#                     """
#                     Write a concise summary of the following:
#                     "{text}"
#                     CONCISE SUMMARY:                
#                 """
#                 )

#                 first_summary_chain = first_summary_prompt | llm | StrOutputParser()

#                 summary = first_summary_chain.invoke(
#                     {"text": docs[0].page_content},
#                 )

#                 refine_prompt = ChatPromptTemplate.from_template(
#                     """
#                     Your job is to produce a final summary.
#                     We have provided an existing summary up to a certain point: {existing_summary}
#                     We have the opportunity to refine the existing summary (only if needed) with some more context below.
#                     ------------
#                     {context}
#                     ------------
#                     Given the new context, refine the original summary.
#                     If the context isn't useful, RETURN the original summary.
#                     """
#                 )

#                 refine_chain = refine_prompt | llm | StrOutputParser()

#                 with st.status("Summarizing...") as status:
#                     for i, doc in enumerate(docs[1:]):
#                         status.update(label=f"Processing document {i+1}/{len(docs)-1} ")
#                         summary = refine_chain.invoke(
#                             {
#                                 "existing_summary": summary,
#                                 "context": doc.page_content,
#                             }
#                         )
#                 st.write(summary)
#             except Exception as e:
#                 st.error(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#                 logger.error(f"Error generating summary: {e}")

#     with qa_tab:
#         try:
#             retriever = embed_file(transcript_path)
#         except Exception as e:
#             st.error(f"ì„ë² ë”© íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#             logger.error(f"Error embedding file {transcript_path}: {e}")
#             st.stop()

#         # ì‚¬ìš©ìë¡œë¶€í„° ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
#         user_question = st.text_input("Ask a question about the transcript:")
#         if user_question:
#             try:
#                 # ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
#                 docs = retriever.get_relevant_documents(user_question)
#                 if docs:
#                     # ê´€ë ¨ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
#                     context = "\n\n".join(doc.page_content for doc in docs)
#                     qa_prompt = ChatPromptTemplate.from_template(
#                         """
#                         You are a helpful assistant.

#                         Use the following context to answer the question.

#                         Context:
#                         {context}

#                         Question:
#                         {question}

#                         Answer:
#                     """
#                     )
#                     qa_chain = qa_prompt | llm
#                     answer = qa_chain.invoke(
#                         {
#                             "context": context,
#                             "question": user_question,
#                         }
#                     )
#                     st.write(answer)
#                 else:
#                     st.write("No relevant information found.")
#             except Exception as e:
#                 st.error(f"Q&A ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
#                 logger.error(f"Error during Q&A: {e}")
